# -*- coding: utf-8 -*-
"""Image_Extraction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EndkkrRTpmDBy3bsAC82NDMM_jlDecvG

# Importing and Installing
"""

!rm -rf waymo-od > /dev/null
!git clone https://github.com/waymo-research/waymo-open-dataset.git waymo-od
!cd waymo-od && git branch -a
!cd waymo-od && git checkout remotes/origin/master
!pip3 install --upgrade pip

!pip3 install waymo-open-dataset-tf-2-6-0

!wget https://ff940237a54efe7ec8352831fe87a66c53b4edf3a13ff2d742f5792-apidata.googleusercontent.com/download/storage/v1/b/waymo_open_dataset_v_1_4_1/o/archived_files%2Ftraining%2Ftraining_0004.tar?jk=AahUMls2amHaoCh66FWxOhDmIlJzwv3RjVS8lCnwHIDKQJfbjgLHFCZFrTJf_TTzjILLgalvFzmhO9558e3PJGkxO-wbfDCotnmepn6rEPJRO8didsRS_cxzYD11OZOW3h8TGwlOrVmaH0OMZS10QpzM4XZR772EHobrXrqwGfF7wxAQnMudQztruCW0VNNjWECjOdATuX8_BvR4r4ZGA0cVuVRA0d6EnVYmuN65dhIR2oZ6bPBFoWsvZpmW8CzMIVcry5Ra8KbCt0fAF8p8PXpQLS1RpYta2M1D6BM2MibAYHbZXUmqBmu61iHfLErB3gZTA7Z1yeNk4L4v-lMejKcfPtGtgWeyvQeISDA3UPZ5246N0qi_5i4O-tv7FF9dSBo68OlUQrEa_-bF9HzS7Z_vkHb6e8OpyQAOcznqHMRDJQ8WF62gFLMYk5jcywilqGLERWpuscKAl4zUCHMWd-ulAcUVzkoUSgDYrWaeP64s3wQDtWBp_gi9w428RUT_7iL73eSlVjLXhzKi1Fgk4Y-FO8tBnl-6xGsHmDEwSDqc8UldUdqccEuCip-EQ7BxBo3hNGDQIGFv5mG6ss7BfuMTCtnpHoFT1R0g4GeSFuItAA0NA0-OXANmVgkRlvbzXGQKNUmuyICWL8xtG2PZ-vNqX7kw1uZKJGYmNKwQtqGsQAWKBcj48MMCsq6utXEEQIelNdDt4YW3jI1HnLgI9DouDlEjOsfT_4kXeOpzV5ExX_HefyzJ1AapcPwpHjf461bV1W73G8UENnDCS1C_tGOLs-XMD9kaAW9A6GSqEL4ABofKL_i1KsKuZHhaAghxaihoKPRl0wPipOe4fAn1QacdIxwpVGAa4o8uDq2ZU3s058eT1O2ErpnuGgdAiatKXNxKn7pNaSdSROcu5i5pzjBpUgrrMm8uvSl61zXvpnp3X3dne74iDU_813QdFxo5Nijo7WWjrjbSseW3V6No8rpJopSxEyVrA0BkShzChcg5gzyYXH-msh7ksjyp2T2GbHwNg-Ulf57AjL6O8DPMxZxGHz5aDgnt9RxaANN0yWrYWZ1-tB9dMABLxLlSejJV9MM_J1S8rdRoBt3nboTiE1mAp3NhXJRfXCD2vUeTpPJngf9T2C35rcCr5Kqsrr40Qc74Gw-ssOC-4cx3GmGw_RaYAOb9Oa5hoBzAMRf0bysCWy5WKPgywIEInE8Hm1SjwZ_7TgmGhluxgl3euW9iBVM&isca=1

from waymo_open_dataset.utils import range_image_utils
from waymo_open_dataset.utils import transform_utils
from waymo_open_dataset.utils import  frame_utils
from waymo_open_dataset import dataset_pb2 as open_dataset
from waymo_open_dataset.protos import segmentation_metrics_pb2
from waymo_open_dataset.protos import segmentation_submission_pb2

import tarfile
import os
import tensorflow.compat.v1 as tf
import math
import numpy as np
import itertools
import sys
import matplotlib.pyplot as plt

tf.enable_eager_execution()

file = tarfile.open('large_dataset.tar')
  
# extracting file
file.extractall('./Combined_dataset')

file.close()

os.remove('large_dataset.tar')

directory = 'Combined_dataset'

label_list = []
image_list = []

done_count = 0

for filename in os.listdir(directory):
  FILENAME = os.path.join(directory, filename)

  dataset = tf.data.TFRecordDataset(FILENAME, compression_type='')

  try:
    for data in dataset:
        frame = open_dataset.Frame()
        frame.ParseFromString(bytearray(data.numpy()))

        for i in range(len(frame.images)):
            panoptic_image = frame.images[i].camera_segmentation_label.panoptic_label
        try:
            label = tf.image.decode_png(panoptic_image)
            img = tf.image.decode_jpeg(frame.images[i].image)
            label_list.append(label.numpy())
            image_list.append(img.numpy())
        except:
            pass

    done_count += 1
      
  except Exception as e:
      print("Error:", e)

  print(FILENAME, "Done!")
  print(len(label_list), len(image_list))

len(label_list), len(image_list)

plt.imshow(image_list[0])

plt.imshow(label_list[0][:,:,0])

sys.getsizeof(image_list[0])

5103496 * 394

sys.getsizeof(label_list[0])

image_list[1].shape

img_arr = np.array(image_list)

label_arr = np.array(label_list)

sys.getsizeof(img_arr), sys.getsizeof(label_arr)

np.save('image_array', img_arr)

np.save('label_array', label_arr)

"""# Using Saved Data to train Model"""

image_arr = np.load('/content/drive/MyDrive/Major project/image_array.npy')
label_arr = np.load('/content/drive/MyDrive/Major project/label_array.npy')

image_arr.shape, label_arr.shape

plt.imshow(label_arr[0][:,:,0])

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(image_arr[:30], label_arr[:30], test_size = 0.2, random_state = 1)

X_train.shape, X_test.shape

category_map = {}
values = [  0,   3,   7,   8,  11,  15,  19,  27,  35,  50,  54,  58,  62,
        66,  70,  74,  78,  82,  85,  89,  93,  97, 101, 105, 109]

for i in range(25):
  category_map[values[i]] = i

category_map

for key, val in category_map.items():
  y_train[y_train==key]=val

np.unique(y_train)

for key, val in category_map.items():
  y_test[y_test==key]=val

np.unique(y_test)

from keras.utils import to_categorical

y_train_cat = to_categorical(y_train, num_classes=25, dtype = 'bool')

y_train_cat.shape

y_test_cat = to_categorical(y_test, num_classes=25, dtype = 'bool')
y_test_cat.shape

X_train_scaled = X_train / 255.0
X_test_scaled = X_test / 255.0

X_train.shape, X_test.shape

from keras.models import *
from keras.layers import *
from keras.optimizers import *

def unet(pretrained_weights = None, input_size = (256,256,1)):
    inputs = Input(input_size)
    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)
    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)
    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)
    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)
    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)
    drop4 = Dropout(0.5)(conv4)
    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)

    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)
    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)
    drop5 = Dropout(0.5)(conv5)

    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))
    merge6 = concatenate([drop4,up6], axis = 3)
    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)
    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)

    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))
    merge7 = concatenate([conv3,up7], axis = 3)
    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)
    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)

    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))
    merge8 = concatenate([conv2,up8], axis = 3)
    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)
    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)

    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))
    merge9 = concatenate([conv1,up9], axis = 3)
    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)
    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)
    conv10 = Conv2D(25, 1, activation = 'softmax')(conv9)

    model = Model(inputs = inputs, outputs = conv10)
    
    model.summary()

    return model

X_train_scaled.shape

X_train_resized = np.resize(X_train_scaled, (24, 896, 1920, 3))
X_test_resized = np.resize(X_test_scaled, (6, 896, 1920, 3))

y_train_resized = np.resize(y_train_cat, (24, 896, 1920, 25))
y_test_resized = np.resize(y_test_cat, (6, 896, 1920, 25))

model = unet(input_size = (896, 1920, 3))

model.compile(optimizer = Adam(lr = 1e-4), loss = 'categorical_crossentropy', metrics = ['accuracy'])

X_train_resized.shape, X_test_resized.shape

np.save('/content/drive/MyDrive/Major project/X_train_final.npy', X_train_resized)
np.save('/content/drive/MyDrive/Major project/X_test_final.npy', X_test_resized)
np.save('/content/drive/MyDrive/Major project/y_train_final.npy', y_train_resized)
np.save('/content/drive/MyDrive/Major project/y_test_final.npy', y_test_resized)

X_train_resized = np.load('/content/drive/MyDrive/Major project/X_train_final.npy')
X_test_resized = np.load('/content/drive/MyDrive/Major project/X_test_final.npy')
y_train_resized = np.load('/content/drive/MyDrive/Major project/y_train_final.npy')
y_test_resized = np.load('/content/drive/MyDrive/Major project/y_test_final.npy')

history = model.fit(X_train_resized, y_train_resized, epochs = 20, batch_size = 4, validation_data = (X_test_resized, y_test_resized))

