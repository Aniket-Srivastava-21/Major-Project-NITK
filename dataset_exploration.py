# -*- coding: utf-8 -*-
"""Dataset_exploration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wpkcf84ZefSzx8Rugb6yrZaDSnMogrw-

# Package Installation

## Install waymo_open_dataset package
"""

!rm -rf waymo-od > /dev/null
!git clone https://github.com/waymo-research/waymo-open-dataset.git waymo-od
!cd waymo-od && git branch -a
!cd waymo-od && git checkout remotes/origin/master
!pip3 install --upgrade pip

!pip3 install waymo-open-dataset-tf-2-6-0

"""## Downloading Archived Files"""

!wget https://ff8b5037a0de98bb49bb9ccfa0ab0227aad06c0f03288857c186da4-apidata.googleusercontent.com/download/storage/v1/b/waymo_open_dataset_v_1_4_1/o/archived_files%2Ftraining%2Ftraining_0000.tar?jk=AahUMlu3yLz5nh-H94ZhgW19mXcXVOHvrdn44Qug7D0UHeFxLwBoofc-Nsr4bU6h24g5RmT5cUngErBR2H0K0vUfHy5WMv6CyS1dxhApDW8RhXx5Nsl6rxhwC-edA6nQuNGbzcdowH6w5qESywl9zDceHQg5HL6paYKVpQodr39cvK0uCBrY5pC9BpwVAhIIZN3_IMTbRb3P7CSu0h4IRX_SayrSNp6gDb2-vuMU7K6ZgA_m-svMsvAaqJ8KeGdH6RFCtZIbeRTQJ2UN4CLom6AE3PnrWdVzbaXKiCyr2AS0W_Ir0w4_Q2uEDaWT3H8ZcuC4zExZGfaTZJhcxWTe8F2rPefQjYm4jthcFftJzx131L_HS5UQeVAml9nUMWUA6Fbb4WUGJhA4d8sStKkVfxTjltiz-tbDHA1YCjqhPZALONEaC_hWO5tX83eopdE5_8hJuVSZm6JGedShU4gLLbEfMFxQvsD21NlCavxTcW9ERgDsuWddt7RV6zIlMVc-X6Je8FzW-h_jmwiuK7CuP--lXr0na79g0A9YF6KPXwiU5BkWoFyToMbDerJLQ4SyUeBVJqk7DPKgxvuevcVBxToFvJk4nL_aKRQjIuKvpVZlfQJ_UbUpQDARSnJ6LJ7AZTD8FqedEilUr5sfhqntdrcwVbwMYXzRbHkrAp0476w7P6NKqHUO0otS5aNFcYKGx0kYClE7INDQTLO2aNL8qmEPBOPuDreU5mSAa7pu6zWnsetJ4VYmgAx0O_0LWaSyuaj_tLwxHLCrhoD8hnuJiG1tSclApqy6ExcjLAGFdpJ6wRaRffAru7o9EjqdPkpA9NPVyd9bVmVYe89xODkyS8Er5mnw8aeu1qfCbahaxomYImMMDpXMzOMKd6orVGzXgZZtt1sZUwVR_WNreNKiVbnMBTg3qAi9j-QLReIqsxUqx7q7o_sf8sSrA_X7Qh3Kjck4OJ3x5zD6I0FE6DgSwRei6VNuxNwd-sIfT7bDWrhPhEqtY0QjUgw7Dq9W_h1tcWcOxwkt2sBhVycRNelfOUb1ACmfndktI1u5vOl1Dm-yCi7wWZxmfVuTje4mZ5WJNXv-Hg6K45eAO_AxXipkj_7BRWX8kWEn5qfViJ91o33qXl12Og4_hDz9Sb1ldDqKwCN_HuomJigiA5n594k7Q9PpXTUXxBLZ6q9DvID3eq-dsieX5T0heKmjteJ7qZZrrAn29WGKtGcFmP1zycGnYkgHQt3VIq0KezGSpOzRLhInglg3_lU4D7hX-Q&isca=1

"""### Extracting downloaded .tar file using tarfile package"""

import tarfile

file = tarfile.open('large_dataset.tar')
  
# extracting file
file.extractall('./Combined_dataset')

file.close()

import os
import tensorflow.compat.v1 as tf
import math
import numpy as np
import itertools

tf.enable_eager_execution()

from waymo_open_dataset.utils import range_image_utils
from waymo_open_dataset.utils import transform_utils
from waymo_open_dataset.utils import  frame_utils
from waymo_open_dataset import dataset_pb2 as open_dataset

import matplotlib.pyplot as plt

tf.enable_eager_execution()

from waymo_open_dataset.protos import segmentation_metrics_pb2
from waymo_open_dataset.protos import segmentation_submission_pb2

"""# Imports and global definitions"""

# mount drive 
# from google.colab import drive
# drive.mount('/content/drive')

!wget https://ffed4984ef6f2b9ea8ddd7699505434e18651940d0491bcc197b347-apidata.googleusercontent.com/download/storage/v1/b/waymo_open_dataset_v_1_4_1/o/individual_files%2Ftraining%2Fsegment-10455472356147194054_1560_000_1580_000_with_camera_labels.tfrecord?jk=AahUMls6GNcaFCHsvcSuoil3merXSkR5yYPe5ySOSYHde5D7jK3qEzFw1JpFp1aT-WB6NiT7xw1DBSOd0A-qZrXKf5Jl5tPShwh9VhfyUfwZ15uxu1fP3kTZV-0MHeP-LTk8AmNNNxys9OfJy_dFgYjWGfth5-cLv5pSTb5oLli97cIUsHvakDHusmDS-0wwobXHAdEh-0gO2l8yWwX3LkmnuYjfsztj0_hYmdWRxGkDmzmN7y2PZqrjM14ZJ6D6vONSprNSTtK7zGGld6LcbUK1jxr40BHKc2s5KprtdZyMgokuS9lfy_M_FvGO8g1Tvm_ihposM_dheDHIeGaoKmT4t3heXQCyLqhf9xPcGPbPGw6n4AcPPStXgaoDr8D_Ur25ZgZEZBwTpq7cUm0-FStfkXOqk0e_ZWP9ckrm3oDuiCu8XXCLCZaEa2IO8mV9A5bF52I0PY2sEvRlk14C9N9M3dT3GycuyDqe1PtL_bse0OstW3VNf7mbZS3TWafvxJhiBlH6jAqK5TPsx2zSivnq6I3TzOfhNUi0T8Q65_FKGoLV_EecfiMQ8s0qakvNnte82-GAnklwV46YR5hPhP-uj0msJUtT6FMHiNXo2776-FZbb8uDpOjw7fqhgZjaf4fEMexQwX6pGN-P699A7KKlHm0yyiuV5vsnICIS0edlfBVpvYxKMGLXLtFl6RIAWhjtP-xyeb3ANT06jHrRs0BLTbhdxfS0Y2gJRsEWPBLTcYxR-66eXhsLIoZ_ZOc2P23vg0PGQ_yStKRY_g-rCcEZ-mB-rDyq0gw32KW3aJ6okpRDXvfRX046tlLafcD-TTyweS5ZDz5oSRzdY5GlryubDFW3CLVmrSoUlrdUEdI7wLTpX5VYXh2Wo3Bai92V7jQt6J1cNEY7OLNH1k8Nic5PKQ6OMb5WVmBLGAl7Ax3LYzAjZm5zfd1UZ1yC4wUiAitgokfNsCebyfCqVC79FLMAfmnR1eCERfrzRpVxdZz-x4IPUSfa6y-je91KmxBYt7Ct7aMwYxaaC8bDzZS2zk4u-gA5tmLmCWEkYln0bmZB81nZBZdbmPmXiOmChOCfwaCShbowRnkSuA6wX0PAM2flYNxgKgOxHiRPlRkEAIhN6ZKc0LnTjzlQU3mltK8IxqNngrTb7UMhO0gwM2XuqHS-N0bqrzfxj5qxtI9Zj1IVNj9-hjAETvrd0ie9ix7lAcNEW90jIJEmJDcy0c1c7-UZnk1b0Wt8kmwStlBMgVGI0LDPQHXS909GE6NyJ8TrEWlzQ_6dkRQC4-edubmwvIvqEmFogqOUdXigaIBA1LmIbODeY656Ei9j9nBvRHVzZH4lDJ9I_Wax9ltjBzZ2SRJy3VK0y7Q72h_O-ZJtTIjr9HRdceFqglZYB6rOEd88PbLwqYT3M-5fS_TFvVYHsww9RylDEUUqEjH4x5FaOxiPbhHzPrUmATGjNK6HxKzgjcqlsezsReSPWwulBJ4Gvj8NZ7c9e3o3jTTn7dOFDuJQZXrhQjTFP-ZD&isca=1

# Data location. Please edit.

# A tfrecord containing tf.Example protos as downloaded from the Waymo dataset
# webpage.

# Replace this path with your own tfrecords.
FILENAME = '/content/dataset.tfrecord'

"""# Read 3D semantic segmentation labels from Frame proto
 Note that only a subset of the frames have 3d semseg labels.
"""

import sys

dataset = tf.data.TFRecordDataset(FILENAME, compression_type='')
cnt = 0
cnt2 = 0
frames = []
all_frames = []
for data in dataset:
    frame = open_dataset.Frame()
    frame.ParseFromString(bytearray(data.numpy()))
    all_frames.append(frame)
    cnt += 1
    if frame.lasers[0].ri_return1.segmentation_label_compressed:
        frames.append(frame)
        cnt2 += 1
print("cnt:", cnt)
print("cnt2:", cnt2)

frame = frames[0]
print(frame.context)

print(frame.timestamp_micros)

frame.pose

"""## Exploring Frame Images Class"""

plt.figure(figsize = (25, 15))
for i in range(len(frame.images)):
    print(sys.getsizeof(frame.images[i].image))
    img = frame.images[i].image
    plt.subplot(2, 3, i+1)
    plt.imshow(tf.image.decode_jpeg(img))

frame.images[0].pose

frame.images[0].velocity

frame.images[0].shutter

label_list = []
image_list = []
for frame_ in all_frames:
  for i in range(len(frame_.images)):
    panoptic_image = frame_.images[i].camera_segmentation_label.panoptic_label
    try:
      label = tf.image.decode_png(panoptic_image)
      img = tf.image.decode_jpeg(frame_.images[i].image)
      label_list.append(label)
      image_list.append(img)
    except:
      pass

print(len(label_list))
len(image_list)

"""## Visualizing the Panoptic Labels"""

import random

indices = random.sample(range(100), 5)

plt.figure(figsize = (16, 30))
for i, index in enumerate(indices):
  plt.subplot(5, 2, i * 2 + 1)
  plt.title("Image")
  plt.imshow(image_list[index])

  plt.subplot(5, 2, i * 2 + 2)
  plt.title("Label")
  plt.imshow(label_list[index][:,:,0])

np.unique(label_list[0].numpy())

frame.laser_labels[1]

(range_images, camera_projections, segmentation_labels,
 range_image_top_pose) = frame_utils.parse_range_image_and_camera_projection(
    frame)

segmentation_labels.keys()

print(segmentation_labels[open_dataset.LaserName.TOP][0].shape.dims)

"""## Lidar data"""

plt.figure(figsize=(64, 20))
def plot_range_image_helper(data, name, layout, vmin = 0, vmax=1, cmap='gray'):
  """Plots range image.

  Args:
    data: range image data
    name: the image title
    layout: plt layout
    vmin: minimum value of the passed data
    vmax: maximum value of the passed data
    cmap: color map
  """
  plt.subplot(*layout)
  plt.imshow(data, cmap=cmap, vmin=vmin, vmax=vmax)
  plt.title(name)
  plt.grid(False)
  plt.axis('off')

def get_range_image(laser_name, return_index):
  """Returns range image given a laser name and its return index."""
  return range_images[laser_name][return_index]

def show_range_image(range_image, layout_index_start = 1):
  """Shows range image.

  Args:
    range_image: the range image data from a given lidar of type MatrixFloat.
    layout_index_start: layout offset
  """
  range_image_tensor = tf.convert_to_tensor(range_image.data)
  range_image_tensor = tf.reshape(range_image_tensor, range_image.shape.dims)
  lidar_image_mask = tf.greater_equal(range_image_tensor, 0)
  range_image_tensor = tf.where(lidar_image_mask, range_image_tensor,
                                tf.ones_like(range_image_tensor) * 1e10)
  range_image_range = range_image_tensor[...,0] 
  range_image_intensity = range_image_tensor[...,1]
  range_image_elongation = range_image_tensor[...,2]
  plot_range_image_helper(range_image_range.numpy(), 'range',
                   [8, 1, layout_index_start], vmax=75, cmap='gray')
  plot_range_image_helper(range_image_intensity.numpy(), 'intensity',
                   [8, 1, layout_index_start + 1], vmax=1.5, cmap='gray')
  plot_range_image_helper(range_image_elongation.numpy(), 'elongation',
                   [8, 1, layout_index_start + 2], vmax=1.5, cmap='gray')
frame.lasers.sort(key=lambda laser: laser.name)
# 1st return for TOP sensor
show_range_image(get_range_image(open_dataset.LaserName.TOP, 0), 1)
# 2nd return for TOP sensor

show_range_image(get_range_image(open_dataset.LaserName.TOP, 1), 4)

"""##Point Cloud Conversion and Visualization"""

points, cp_points = frame_utils.convert_range_image_to_point_cloud(
    frame,
    range_images,
    camera_projections,
    range_image_top_pose)
points_ri2, cp_points_ri2 = frame_utils.convert_range_image_to_point_cloud(
    frame,
    range_images,
    camera_projections,
    range_image_top_pose,
    ri_index=1)

# 3d points in vehicle frame.
points_all = np.concatenate(points, axis=0)
points_all_ri2 = np.concatenate(points_ri2, axis=0)
# camera projection corresponding to each point.
cp_points_all = np.concatenate(cp_points, axis=0)
cp_points_all_ri2 = np.concatenate(cp_points_ri2, axis=0)

"""###Examine number of points in each lidar sensor.

First return.
"""

print(points_all.shape)
print(cp_points_all.shape)
print(points_all[0:2])
for i in range(5):
  print(points[i].shape)
  print(cp_points[i].shape)

"""Second return."""

print(points_all_ri2.shape)
print(cp_points_all_ri2.shape)
print(points_all_ri2[0:2])
for i in range(5):
  print(points_ri2[i].shape)
  print(cp_points_ri2[i].shape)

"""# Visualize Segmentation Labels in Range Images"""

plt.figure(figsize=(64, 20))
def plot_range_image_helper(data, name, layout, vmin = 0, vmax=1, cmap='gray'):
  """Plots range image.

  Args:
    data: range image data
    name: the image title
    layout: plt layout
    vmin: minimum value of the passed data
    vmax: maximum value of the passed data
    cmap: color map
  """
  plt.subplot(*layout)
  plt.imshow(data, cmap=cmap, vmin=vmin, vmax=vmax)
  plt.title(name)
  plt.grid(False)
  plt.axis('off')

def get_semseg_label_image(laser_name, return_index):
  """Returns semseg label image given a laser name and its return index."""
  return segmentation_labels[laser_name][return_index]

def show_semseg_label_image(semseg_label_image, layout_index_start = 1):
  """Shows range image.

  Args:
    show_semseg_label_image: the semseg label data of type MatrixInt32.
    layout_index_start: layout offset
  """
  semseg_label_image_tensor = tf.convert_to_tensor(semseg_label_image.data)
  semseg_label_image_tensor = tf.reshape(
      semseg_label_image_tensor, semseg_label_image.shape.dims)
  instance_id_image = semseg_label_image_tensor[...,0] 
  semantic_class_image = semseg_label_image_tensor[...,1]
  plot_range_image_helper(instance_id_image.numpy(), 'instance id',
                   [8, 1, layout_index_start], vmin=-1, vmax=200, cmap='Paired')
  plot_range_image_helper(semantic_class_image.numpy(), 'semantic class',
                   [8, 1, layout_index_start + 1], vmin=0, vmax=22, cmap='tab20')

frame.lasers.sort(key=lambda laser: laser.name)
show_semseg_label_image(get_semseg_label_image(open_dataset.LaserName.TOP, 0), 1)
show_semseg_label_image(get_semseg_label_image(open_dataset.LaserName.TOP, 1), 4)

"""# Point Cloud Conversion and Visualization"""

def convert_range_image_to_point_cloud_labels(frame,
                                              range_images,
                                              segmentation_labels,
                                              ri_index=0):
  """Convert segmentation labels from range images to point clouds.

  Args:
    frame: open dataset frame
    range_images: A dict of {laser_name, [range_image_first_return,
       range_image_second_return]}.
    segmentation_labels: A dict of {laser_name, [range_image_first_return,
       range_image_second_return]}.
    ri_index: 0 for the first return, 1 for the second return.

  Returns:
    point_labels: {[N, 2]} list of 3d lidar points's segmentation labels. 0 for
      points that are not labeled.
  """
  calibrations = sorted(frame.context.laser_calibrations, key=lambda c: c.name)
  point_labels = []
  for c in calibrations:
    range_image = range_images[c.name][ri_index]
    range_image_tensor = tf.reshape(
        tf.convert_to_tensor(range_image.data), range_image.shape.dims)
    range_image_mask = range_image_tensor[..., 0] > 0

    if c.name in segmentation_labels:
      sl = segmentation_labels[c.name][ri_index]
      sl_tensor = tf.reshape(tf.convert_to_tensor(sl.data), sl.shape.dims)
      sl_points_tensor = tf.gather_nd(sl_tensor, tf.where(range_image_mask))
    else:
      num_valid_point = tf.math.reduce_sum(tf.cast(range_image_mask, tf.int32))
      sl_points_tensor = tf.zeros([num_valid_point, 2], dtype=tf.int32)
      
    point_labels.append(sl_points_tensor.numpy())
  return point_labels

points, cp_points = frame_utils.convert_range_image_to_point_cloud(
    frame, range_images, camera_projections, range_image_top_pose)
points_ri2, cp_points_ri2 = frame_utils.convert_range_image_to_point_cloud(
    frame, range_images, camera_projections, range_image_top_pose, ri_index=1)

point_labels = convert_range_image_to_point_cloud_labels(
    frame, range_images, segmentation_labels)
point_labels_ri2 = convert_range_image_to_point_cloud_labels(
    frame, range_images, segmentation_labels, ri_index=1)

# 3d points in vehicle frame.
points_all = np.concatenate(points, axis=0)
points_all_ri2 = np.concatenate(points_ri2, axis=0)
# point labels.
point_labels_all = np.concatenate(point_labels, axis=0)
point_labels_all_ri2 = np.concatenate(point_labels_ri2, axis=0)
# camera projection corresponding to each point.
cp_points_all = np.concatenate(cp_points, axis=0)
cp_points_all_ri2 = np.concatenate(cp_points_ri2, axis=0)

"""###Show colored point cloud
Example of rendered point clouds (this tutorial does not have visualization capability).
"""

from IPython.display import Image, display
display(Image('/content/waymo-od/tutorial/3d_semseg_points.png'))

frame.laser_labels

frame.projected_lidar_labels

frame.camera_labels

frame.no_label_zones

"""### Script to gather all the images with their respective panoptic labels"""

directory = 'Combined_dataset'

label_list = []
image_list = []

done_count = 0

for filename in os.listdir(directory):
  FILENAME = os.path.join(directory, filename)

  dataset = tf.data.TFRecordDataset(FILENAME, compression_type='')

  try:
    for data in dataset:
        frame = open_dataset.Frame()
        frame.ParseFromString(bytearray(data.numpy()))

        for i in range(len(frame.images)):
            panoptic_image = frame.images[i].camera_segmentation_label.panoptic_label
        try:
            label = tf.image.decode_png(panoptic_image)
            img = tf.image.decode_jpeg(frame.images[i].image)
            label_list.append(label.numpy())
            image_list.append(img.numpy())
        except:
            pass
      
  except Exception as e:
      print("Error:", e)

  print(FILENAME, "Done!")
  print(len(label_list), len(image_list))

len(label_list), len(image_list)

for i in range(len(image_list)):
  if(i == 0):
    continue
  if image_list[i-1].shape != image_list[i].shape:
    print(image_list[i].shape)
    break

plt.imshow(image_list[0])

plt.imshow(label_list[0][:,:,0])

sys.getsizeof(image_list[0]) * sys.getsizeof(image_list)

label_list[0].shape

image_arr = np.array(image_list)
label_arr = np.array(label_list)

image_arr.shape, label_arr.shape

"""# Create a dummy submission file for the validation set"""

import zlib

def compress_array(array: np.ndarray, is_int32: bool = False):
  """Compress a numpy array to ZLIP compressed serialized MatrixFloat/Int32.

  Args:
    array: A numpy array.
    is_int32: If true, use MatrixInt32, otherwise use MatrixFloat.

  Returns:
    The compressed bytes.
  """
  if is_int32:
    m = open_dataset.MatrixInt32()
  else:
    m = open_dataset.MatrixFloat()
  m.shape.dims.extend(list(array.shape))
  m.data.extend(array.reshape([-1]).tolist())
  return zlib.compress(m.SerializeToString())

def decompress_array(array_compressed: bytes, is_int32: bool = False):
  """Decompress bytes (of serialized MatrixFloat/Int32) to a numpy array.

  Args:
    array_compressed: bytes.
    is_int32: If true, use MatrixInt32, otherwise use MatrixFloat.

  Returns:
    The decompressed numpy array.
  """
  decompressed = zlib.decompress(array_compressed)
  if is_int32:
    m = open_dataset.MatrixInt32()
    dtype = np.int32
  else:
    m = open_dataset.MatrixFloat()
    dtype = np.float32
  m.ParseFromString(decompressed)
  return np.array(m.data, dtype=dtype).reshape(m.shape.dims)

TOP_LIDAR_ROW_NUM = 64
TOP_LIDAR_COL_NUM = 2650

def get_range_image_point_indexing(range_images, ri_index=0):
  """Get the indices of the valid points (of the TOP lidar) in the range image.

  The order of the points match those from convert_range_image_to_point_cloud
  and convert_range_image_to_point_cloud_labels.

  Args:
    range_images: A dict of {laser_name, [range_image_first_return,
       range_image_second_return]}.
    ri_index: 0 for the first return, 1 for the second return.

  Returns:
    points_indexing_top: (N, 2) col and row indices of the points in the
      TOP lidar.
  """
  points_indexing_top = None
  xgrid, ygrid = np.meshgrid(range(TOP_LIDAR_COL_NUM), range(TOP_LIDAR_ROW_NUM))
  col_row_inds_top = np.stack([xgrid, ygrid], axis=-1)
  range_image = range_images[open_dataset.LaserName.TOP][ri_index]
  range_image_tensor = tf.reshape(
      tf.convert_to_tensor(range_image.data), range_image.shape.dims)
  range_image_mask = range_image_tensor[..., 0] > 0
  points_indexing_top = col_row_inds_top[np.where(range_image_mask)]
  return points_indexing_top

def dummy_semseg_for_one_frame(frame, dummy_class=14):
  """Assign all valid points to a single dummy class.

  Args:
    frame: An Open Dataset Frame proto.
    dummy_class: The class to assign to. Default is 14 (building).

  Returns:
    segmentation_frame: A SegmentationFrame proto.
  """
  (range_images, camera_projections, segmentation_labels,
   range_image_top_pose) = frame_utils.parse_range_image_and_camera_projection(
       frame)
  # Get the col, row indices of the valid points.
  points_indexing_top = get_range_image_point_indexing(range_images, ri_index=0)
  points_indexing_top_ri2 = get_range_image_point_indexing(
      range_images, ri_index=1)
  # Assign the dummy class to all valid points (in the range image)
  range_image_pred = np.zeros(
      (TOP_LIDAR_ROW_NUM, TOP_LIDAR_COL_NUM, 2), dtype=np.int32)
  range_image_pred[points_indexing_top[:, 1],
                   points_indexing_top[:, 0], 1] = dummy_class
  range_image_pred_ri2 = np.zeros(
      (TOP_LIDAR_ROW_NUM, TOP_LIDAR_COL_NUM, 2), dtype=np.int32)
  range_image_pred_ri2[points_indexing_top_ri2[:, 1],
                       points_indexing_top_ri2[:, 0], 1] = dummy_class
  # Construct the SegmentationFrame proto.
  segmentation_frame = segmentation_metrics_pb2.SegmentationFrame()
  segmentation_frame.context_name = frame.context.name
  segmentation_frame.frame_timestamp_micros = frame.timestamp_micros
  laser_semseg = open_dataset.Laser()
  laser_semseg.name = open_dataset.LaserName.TOP
  laser_semseg.ri_return1.segmentation_label_compressed = compress_array(
      range_image_pred, is_int32=True)
  laser_semseg.ri_return2.segmentation_label_compressed = compress_array(
      range_image_pred_ri2, is_int32=True)
  segmentation_frame.segmentation_labels.append(laser_semseg)
  return segmentation_frame

# Create the dummy pred file for the validation set run segments.

# Replace this path with the real path to the WOD validation set folder.
folder_name = '/content/waymo-od/.../validation/'

filenames = [os.path.join(folder_name, x) for x in os.listdir(
    folder_name) if 'tfrecord' in x]
assert(len(filenames) == 202)

segmentation_frame_list = segmentation_metrics_pb2.SegmentationFrameList()
for idx, filename in enumerate(filenames):
  if idx % 10 == 0:
    print('Processing %d/%d run segments...' % (idx, len(filenames)))
  dataset = tf.data.TFRecordDataset(filename, compression_type='')
  for data in dataset:
    frame = open_dataset.Frame()
    frame.ParseFromString(bytearray(data.numpy()))
    if frame.lasers[0].ri_return1.segmentation_label_compressed:
      segmentation_frame = dummy_semseg_for_one_frame(frame)
      segmentation_frame_list.frames.append(segmentation_frame)
print('Total number of frames: ', len(segmentation_frame_list.frames))

# Create the submission file, which can be uploaded to the eval server.
submission = segmentation_submission_pb2.SemanticSegmentationSubmission()
submission.account_name = 'joe@gmail.com'
submission.unique_method_name = 'JoeNet'
submission.affiliation = 'Smith Inc.'
submission.authors.append('Joe Smith')
submission.description = "A dummy method by Joe (val set)."
submission.method_link = 'NA'
submission.sensor_type = 1
submission.number_past_frames_exclude_current = 2
submission.number_future_frames_exclude_current = 0
submission.inference_results.CopyFrom(segmentation_frame_list)

output_filename = '/tmp/wod_semseg_val_set_dummy_pred_submission.bin'
f = open(output_filename, 'wb')
f.write(submission.SerializeToString())
f.close()

"""# Create a dummy submission file for the testing set"""

# Create the dummy pred file for the testing set run segments.

# Replace the paths with the real paths to the WOD testing set folders.
folder_name1 = '/content/waymo-od/.../testing/'
folder_name2 = '/content/waymo-od/.../testing_location/'
filenames1 = [os.path.join(folder_name1, x) for x in os.listdir(
    folder_name1) if 'tfrecord' in x]
filenames2 = [os.path.join(folder_name2, x) for x in os.listdir(
    folder_name2) if 'tfrecord' in x]
filenames = filenames1 + filenames2
print(len(filenames))
assert(len(filenames) == 150)

# Replace this path with the real path. The file is under:
# /waymo-open-dataset/tutorial/ in the github repo.
# Each line of the file is the "<context_name>, <timestamp_micros>" of a frame
# with semseg labels. 
testing_set_frame_file = '/path/3d_semseg_test_set_frames.txt'
context_name_timestamp_tuples = [x.rstrip().split(',') for x in (
    open(testing_set_frame_file, 'r').readlines())]

segmentation_frame_list = segmentation_metrics_pb2.SegmentationFrameList()
for idx, filename in enumerate(filenames):
  if idx % 10 == 0:
    print('Processing %d/%d run segments...' % (idx, len(filenames)))
  dataset = tf.data.TFRecordDataset(filename, compression_type='')
  for data in dataset:
    frame = open_dataset.Frame()
    frame.ParseFromString(bytearray(data.numpy()))
    context_name = frame.context.name
    timestamp = frame.timestamp_micros
    if (context_name, str(timestamp)) in context_name_timestamp_tuples:
      print(context_name, timestamp)
      segmentation_frame = dummy_semseg_for_one_frame(frame)
      segmentation_frame_list.frames.append(segmentation_frame)
print('Total number of frames: ', len(segmentation_frame_list.frames))

# Create the submission file, which can be uploaded to the eval server.
submission = segmentation_submission_pb2.SemanticSegmentationSubmission()
submission.account_name = 'joe@gmail.com'
submission.unique_method_name = 'JoeNet'
submission.affiliation = 'Smith Inc.'
submission.authors.append('Joe Smith')
submission.description = "A dummy method by Joe (test set)."
submission.method_link = 'NA'
submission.sensor_type = 1
submission.number_past_frames_exclude_current = 2
submission.number_future_frames_exclude_current = 0
submission.inference_results.CopyFrom(segmentation_frame_list)

output_filename = '/tmp/wod_semseg_test_set_dummy_pred_submission.bin'
f = open(output_filename, 'wb')
f.write(submission.SerializeToString())
f.close()